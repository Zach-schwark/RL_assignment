{"cells":[{"cell_type":"markdown","metadata":{},"source":["# DQN Baseline with the Entire Observation and Action Space"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install gymnasium Grid2Op lightsim2grid wandb stable-baselines3[extra] sb3-contrib"]},{"cell_type":"markdown","metadata":{},"source":["### Observation Space"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oqKiofDcZcFH"},"outputs":[],"source":["#The entire observation Space is in Keys_array\n","\n","keys_array = [\n","    'a_ex', 'a_or', 'active_alert', 'actual_dispatch', 'alert_duration', 'attack_under_alert',\n","    'attention_budget', 'current_step', 'curtailment', 'curtailment_limit',\n","    'curtailment_limit_effective', 'curtailment_limit_mw', 'curtailment_mw', 'day',\n","    'day_of_week', 'delta_time', 'duration_next_maintenance', 'gen_margin_down',\n","    'gen_margin_up', 'gen_p', 'gen_p_before_curtail', 'gen_q', 'gen_theta', 'gen_v',\n","    'hour_of_day', 'is_alarm_illegal', 'last_alarm', 'line_status', 'load_p', 'load_q',\n","    'load_theta', 'load_v', 'max_step', 'minute_of_hour', 'month', 'p_ex', 'p_or',\n","    'prod_p', 'prod_q', 'prod_v', 'q_ex', 'q_or', 'rho', 'storage_charge',\n","    'storage_power', 'storage_power_target', 'storage_theta', 'target_dispatch',\n","    'thermal_limit', 'theta_ex', 'theta_or', 'time_before_cooldown_line',\n","    'time_before_cooldown_sub', 'time_next_maintenance', 'time_since_last_alarm',\n","    'time_since_last_alert', 'time_since_last_attack', 'timestep_overflow', 'topo_vect',\n","    'total_number_of_alert', 'v_ex', 'v_or', 'was_alarm_used_after_game_over',\n","    'was_alert_used_after_attack', 'year'\n","]\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Environment - Grid2Op"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":493,"status":"ok","timestamp":1729336118940,"user":{"displayName":"Ashlea Smith","userId":"15470386157281395607"},"user_tz":-120},"id":"VMGO8VYDUFve","outputId":"5b950e7e-fc4b-4f91-f8a7-102d0dedaab9"},"outputs":[],"source":["import gymnasium as gym\n","from gymnasium.spaces import Discrete, MultiDiscrete, Box\n","\n","import grid2op\n","from grid2op import gym_compat\n","from grid2op.Parameters import Parameters\n","from grid2op.Action import PlayableAction\n","from grid2op.Observation import CompleteObservation\n","from grid2op.Reward import L2RPNReward, N1Reward, CombinedScaledReward\n","from grid2op.gym_compat import DiscreteActSpace, BoxGymObsSpace\n","\n","from lightsim2grid import LightSimBackend\n","from collections import Counter\n","\n","\n","\n","# Gymnasium environment wrapper around Grid2Op environment\n","class Gym2OpEnv(gym.Env):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self._backend = LightSimBackend()\n","        self._env_name = \"l2rpn_case14_sandbox\"  # DO NOT CHANGE\n","\n","        action_class = PlayableAction\n","        observation_class = CompleteObservation\n","        reward_class = CombinedScaledReward  # Setup further below\n","\n","        # DO NOT CHANGE Parameters\n","        # See https://grid2op.readthedocs.io/en/latest/parameters.html\n","        p = Parameters()\n","        p.MAX_SUB_CHANGED = 4  # Up to 4 substations can be reconfigured each timestep\n","        p.MAX_LINE_STATUS_CHANGED = 4  # Up to 4 powerline statuses can be changed each timestep\n","\n","        # Make grid2op env\n","        self._g2op_env = grid2op.make(\n","            self._env_name, backend=self._backend, test=False,\n","            action_class=action_class, observation_class=observation_class,\n","            reward_class=reward_class, param=p\n","        )\n","\n","        ##########\n","        # REWARD #\n","        ##########\n","        # NOTE: This reward should not be modified when evaluating RL agent\n","        # See https://grid2op.readthedocs.io/en/latest/reward.html\n","        cr = self._g2op_env.get_reward_instance()\n","        cr.addReward(\"N1\", N1Reward(), 1.0)\n","        cr.addReward(\"L2RPN\", L2RPNReward(), 1.0)\n","        # reward = N1 + L2RPN\n","        cr.initialize(self._g2op_env)\n","        ##########\n","\n","        self._gym_env = gym_compat.GymEnv(self._g2op_env)\n","\n","        self.setup_observations()\n","        self.setup_actions()\n","\n","        self.observation_space = self._gym_env.observation_space\n","        self.action_space = self._gym_env.action_space\n","\n","        self.action_counter = Counter()\n","\n","\n","\n","    # The information i used to get the code for the below 2 functions is fromm the getting started from the Grid2Op github.\n","    # specifcally theis link was useful at helping change the observation and action spaces to use with gymnasium : https://github.com/rte-france/Grid2Op/blob/c71a2dfb824dae7115394266e02cc673c8633a0e/getting_started/11_IntegrationWithExistingRLFrameworks.ipynb\n","    # these links also help explain the observation and action space:\n","        # https://github.com/rte-france/Grid2Op/blob/c71a2dfb824dae7115394266e02cc673c8633a0e/getting_started/02_Observation.ipynb\n","        # https://github.com/rte-france/Grid2Op/blob/c71a2dfb824dae7115394266e02cc673c8633a0e/getting_started/03_Action.ipynb\n","\n","    def setup_observations(self):\n","        #uses the entire observation space\n","        self._gym_env.observation_space.close()\n","        \n","        self._gym_env.observation_space = BoxGymObsSpace(self._g2op_env.observation_space)\n","        \n","\n","    def setup_actions(self):\n","        #Uses the entire action space\n","        self._gym_env.action_space = DiscreteActSpace(self._g2op_env.action_space)\n","\n","       \n","\n","    def reset(self, seed=None):\n","        return self._gym_env.reset(seed=seed, options=None)\n","\n","    def step(self, action):\n","        action_key = tuple(action) if isinstance(action, np.ndarray) else action\n","        self.action_counter[action_key] += 1\n","        return self._gym_env.step(action)\n","\n","    def get_action_distribution(self):\n","        total = sum(self.action_counter.values())\n","        return {action: count / total for action, count in self.action_counter.items()}\n","\n","    def reset_action_counter(self):\n","        self.action_counter = Counter()\n","\n","\n","    def render(self):\n","        \n","        return self._gym_env.render()\n","\n","\n","def main():\n","    # Random agent interacting in environment #\n","\n","    max_steps = 100\n","\n","    env = Gym2OpEnv()\n","\n","    print(\"#####################\")\n","    print(\"# OBSERVATION SPACE #\")\n","    print(\"#####################\")\n","    print(env.observation_space)\n","    print(\"#####################\\n\")\n","\n","    print(\"#####################\")\n","    print(\"#   ACTION SPACE    #\")\n","    print(\"#####################\")\n","    print(env.action_space)\n","    print(\"#####################\\n\\n\")\n","\n","    curr_step = 0\n","    curr_return = 0\n","\n","    is_done = False\n","    obs, info = env.reset()\n","    print(f\"step = {curr_step} (reset):\")\n","    print(f\"\\t obs = {obs}\")\n","    print(f\"\\t info = {info}\\n\\n\")\n","\n","    while not is_done and curr_step < max_steps:\n","        action = env.action_space.sample()\n","        obs, reward, terminated, truncated, info = env.step(action)\n","\n","        curr_step += 1\n","        curr_return += reward\n","        is_done = terminated or truncated\n","\n","        print(f\"step = {curr_step}: \")\n","        print(f\"\\t obs = {obs}\")\n","        print(f\"\\t reward = {reward}\")\n","        print(f\"\\t terminated = {terminated}\")\n","        print(f\"\\t truncated = {truncated}\")\n","        print(f\"\\t info = {info}\")\n","\n","        # Some actions are invalid (see: https://grid2op.readthedocs.io/en/latest/action.html#illegal-vs-ambiguous)\n","        # Invalid actions are replaced with 'do nothing' action\n","        is_action_valid = not (info[\"is_illegal\"] or info[\"is_ambiguous\"])\n","        print(f\"\\t is action valid = {is_action_valid}\")\n","        if not is_action_valid:\n","            print(f\"\\t\\t reason = {info['exception']}\")\n","        print(\"\\n\")\n","\n","    print(\"###########\")\n","    print(\"# SUMMARY #\")\n","    print(\"###########\")\n","    print(f\"return = {curr_return}\")\n","    print(f\"total steps = {curr_step}\")\n","    print(\"###########\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1729336098550,"user":{"displayName":"Ashlea Smith","userId":"15470386157281395607"},"user_tz":-120},"id":"0J7ik0pC_gda"},"outputs":[],"source":["#Analyzing the action distrubtion after training\n","def analyze_action_distribution(env, model, episodes = 1000):\n","    env.reset_action_counter()\n","    for _ in range(episodes):\n","        obs, _ = env.reset()\n","        done = False\n","        while not done:\n","            action, _states = model.predict(obs, deterministic = True)\n","            obs, reward, done,terminated, truncated, info = env.step(action)\n","            done = terminated or truncated\n","    distribution = env.get_action_distribution()\n","    return distribution"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":384},"executionInfo":{"elapsed":1138,"status":"error","timestamp":1729340463838,"user":{"displayName":"Ashlea Smith","userId":"15470386157281395607"},"user_tz":-120},"id":"hW9HPGhPUpN0","outputId":"e92c984c-d23f-428a-9864-df6485a0b726"},"outputs":[],"source":["import gymnasium as gym\n","import grid2op\n","from stable_baselines3.common.monitor import Monitor\n","import wandb\n","import sys\n","sys.path.insert(0,\"./\") # this makes it possible to import the wrapper since the env.py is in it's own folder\n","#from provided_wrapper.env import Gym2OpEnv\n","from stable_baselines3 import DQN\n","from wandb.integration.sb3 import WandbCallback\n","import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","from sb3_contrib import QRDQN\n","import numpy as np\n","\n","\n","def main():\n","    env = Gym2OpEnv()\n","    env = Monitor(env)\n","\n","    #Connecting to weights and biases to log the results of the model\n","    run = wandb.init(\n","        project=\"RL_project\",\n","        name = \"DQN_Baseline \",\n","        sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n","        monitor_gym=True,  # auto-upload the videos of agents playing the game\n","        save_code=False,  # optional\n","    )\n","\n","    #Model - standard DQN from stable baselines 3\n","    model = DQN(\"MlpPolicy\", env, verbose =1, tensorboard_log=\"./dqn_grid2op_tensorboard/\")\n","\n","    #Training the model\n","    model.learn(total_timesteps = 100000, log_interval = 10, callback=WandbCallback(gradient_save_freq=100, model_save_path=f\"models/{run.id}\",verbose=2), progress_bar=True)\n","\n","    action_distribution = analyze_action_distribution(env, model)\n","    print(\"Action Distribution:\")\n","    for action, freq in sorted(action_distribution.items(), key=lambda x: x[1], reverse=True):\n","        print(f\"Action {action}: {freq * 100:.2f}%\")\n","\n","    run.finish()\n","\n","    #Run for a few episodes with trained agent\n","    obs,_ = env.reset()\n","\n","    for _ in range(1000):\n","        action, _states = model.predict(obs, deterministic = True)\n","        obs, reward, done, info = env.step(action)\n","        env.render()\n","        if done:\n","            obs = env.reset()\n","\n","    env.close()\n","\n","\n","if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOA/rKKvlH+mbfVCUPU8IQ3","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
