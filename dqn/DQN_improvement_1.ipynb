{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DQN - Improvement 1 \n",
        "## Reducing the observation and action space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1P6VoUb1kiv",
        "outputId": "e822eae7-29c5-4e42-fdab-79e91d9a856c"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium Grid2Op lightsim2grid wandb stable-baselines3[extra] sb3-contrib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Observation Space\n",
        "### Different subsets used to find the best observation space \n",
        "#### attr_to_keep_no_storage was the best and most stable observation space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SkkIefO3lmi",
        "outputId": "05c7312e-eb2d-43e9-e4e9-316485f35e50"
      },
      "outputs": [],
      "source": [
        "keys_array = [\n",
        "    'a_ex', 'a_or', 'active_alert', 'actual_dispatch', 'alert_duration', 'attack_under_alert',\n",
        "    'attention_budget', 'current_step', 'curtailment', 'curtailment_limit',\n",
        "    'curtailment_limit_effective', 'curtailment_limit_mw', 'curtailment_mw', 'day',\n",
        "    'day_of_week', 'delta_time', 'duration_next_maintenance', 'gen_margin_down',\n",
        "    'gen_margin_up', 'gen_p', 'gen_p_before_curtail', 'gen_q', 'gen_theta', 'gen_v',\n",
        "    'hour_of_day', 'is_alarm_illegal', 'last_alarm', 'line_status', 'load_p', 'load_q',\n",
        "    'load_theta', 'load_v', 'max_step', 'minute_of_hour', 'month', 'p_ex', 'p_or',\n",
        "    'prod_p', 'prod_q', 'prod_v', 'q_ex', 'q_or', 'rho', 'storage_charge',\n",
        "    'storage_power', 'storage_power_target', 'storage_theta', 'target_dispatch',\n",
        "    'thermal_limit', 'theta_ex', 'theta_or', 'time_before_cooldown_line',\n",
        "    'time_before_cooldown_sub', 'time_next_maintenance', 'time_since_last_alarm',\n",
        "    'time_since_last_alert', 'time_since_last_attack', 'timestep_overflow', 'topo_vect',\n",
        "    'total_number_of_alert', 'v_ex', 'v_or', 'was_alarm_used_after_game_over',\n",
        "    'was_alert_used_after_attack', 'year'\n",
        "]\n",
        "\n",
        "attr_to_keep_3 = ['a_ex', 'a_or', 'active_alert', 'actual_dispatch', 'alert_duration',\n",
        "    'attention_budget', 'current_step','curtailment', 'curtailment_limit', 'curtailment_limit_effective', 'curtailment_limit_mw','day',\n",
        "    'day_of_week', 'delta_time', 'duration_next_maintenance', 'gen_margin_down',\n",
        "    'gen_margin_up', 'gen_p', 'gen_p_before_curtail', 'gen_q', 'gen_theta', 'gen_v',\n",
        "    'hour_of_day', 'last_alarm', 'line_status', 'load_p', 'load_q',\n",
        "    'load_theta', 'load_v', 'max_step', 'minute_of_hour', 'month', 'p_ex', 'p_or',\n",
        "    'prod_p', 'prod_q', 'prod_v', 'q_ex', 'q_or', 'rho', 'storage_charge',\n",
        "    'storage_power', 'storage_power_target', 'storage_theta', 'target_dispatch',\n",
        "    'thermal_limit', 'theta_ex', 'theta_or', 'time_before_cooldown_line',\n",
        "    'time_before_cooldown_sub', 'time_next_maintenance',\n",
        "    'timestep_overflow', 'topo_vect','v_ex', 'v_or',\n",
        "     'year']#so far the best\n",
        "\n",
        "attr_to_keep_4 = ['a_ex', 'a_or',  'actual_dispatch',\n",
        "    'attention_budget', 'current_step','curtailment', 'curtailment_limit', 'curtailment_limit_effective', 'curtailment_limit_mw', 'curtailment_mw','day',\n",
        "    'day_of_week', 'delta_time', 'duration_next_maintenance', 'gen_margin_down',\n",
        "    'gen_margin_up', 'gen_p', 'gen_p_before_curtail', 'gen_q', 'gen_theta', 'gen_v',\n",
        "    'hour_of_day', 'line_status', 'load_p', 'load_q',\n",
        "    'load_theta', 'load_v', 'max_step', 'minute_of_hour', 'month', 'p_ex', 'p_or',\n",
        "    'prod_p', 'prod_q', 'prod_v', 'q_ex', 'q_or', 'rho', 'storage_charge',\n",
        "    'storage_power', 'storage_power_target', 'storage_theta', 'target_dispatch',\n",
        "    'thermal_limit', 'theta_ex', 'theta_or', 'time_before_cooldown_line',\n",
        "    'time_before_cooldown_sub', 'time_next_maintenance',\n",
        "    'timestep_overflow', 'topo_vect','v_ex', 'v_or',\n",
        "     'year'] #this one is bad so active alert and alert duration is important\n",
        "\n",
        "attr_to_keep_5 = ['a_ex', 'a_or', 'active_alert', 'actual_dispatch', 'alert_duration',\n",
        "    'attention_budget', 'current_step','curtailment', 'curtailment_limit', 'curtailment_limit_effective', 'curtailment_limit_mw', 'curtailment_mw','day',\n",
        "    'day_of_week', 'delta_time', 'duration_next_maintenance', 'gen_margin_down',\n",
        "    'gen_margin_up', 'gen_p', 'gen_p_before_curtail', 'gen_q', 'gen_theta', 'gen_v',\n",
        "    'hour_of_day', 'last_alarm', 'line_status', 'load_p', 'load_q',\n",
        "    'load_theta', 'load_v', 'max_step', 'minute_of_hour', 'month', 'p_ex', 'p_or',\n",
        "    'prod_p', 'prod_q', 'prod_v', 'q_ex', 'q_or', 'rho',\n",
        "    'storage_power', 'storage_power_target', 'storage_theta', 'target_dispatch',\n",
        "    'thermal_limit', 'theta_ex', 'theta_or', 'time_before_cooldown_line',\n",
        "    'time_before_cooldown_sub', 'time_next_maintenance',\n",
        "    'timestep_overflow', 'topo_vect','v_ex', 'v_or',\n",
        "     'year'] #removed storage charged - does very bad\n",
        "\n",
        "attr_to_keep_6 = ['a_ex', 'a_or', 'active_alert', 'actual_dispatch', 'alert_duration',\n",
        "    'attention_budget', 'current_step','curtailment', 'curtailment_limit', 'curtailment_limit_effective', 'curtailment_limit_mw', 'curtailment_mw','day',\n",
        "    'day_of_week', 'delta_time', 'duration_next_maintenance', 'gen_margin_down',\n",
        "    'gen_margin_up', 'gen_p', 'gen_p_before_curtail', 'gen_q', 'gen_theta', 'gen_v',\n",
        "    'hour_of_day', 'last_alarm', 'line_status', 'load_p', 'load_q',\n",
        "    'load_theta', 'load_v', 'max_step', 'minute_of_hour', 'month', 'p_ex', 'p_or',\n",
        "    'prod_p', 'prod_q', 'prod_v', 'q_ex', 'q_or', 'rho', 'storage_charge',\n",
        "    'storage_power', 'storage_power_target', 'storage_theta', 'target_dispatch',\n",
        "    'thermal_limit', 'theta_ex', 'theta_or', 'time_before_cooldown_line',\n",
        "     'time_next_maintenance',\n",
        "    'timestep_overflow', 'topo_vect','v_ex', 'v_or',\n",
        "     'year']#'time_before_cooldown_sub'\n",
        "\n",
        "attr_to_keep_7 = [\"rho\", \"gen_p\", \"load_p\", \"topo_vect\", \"actual_dispatch\"]\n",
        "\n",
        "\n",
        "attr_to_keep = [\n",
        "    'a_ex', 'a_or', 'actual_dispatch',\n",
        "    'attention_budget', 'current_step', 'curtailment', 'curtailment_limit',\n",
        "    'curtailment_limit_effective', 'curtailment_limit_mw', 'curtailment_mw', 'day',\n",
        "    'day_of_week', 'delta_time', 'duration_next_maintenance', 'gen_margin_down',\n",
        "    'gen_margin_up', 'gen_p', 'gen_p_before_curtail', 'gen_q', 'gen_theta', 'gen_v',\n",
        "    'hour_of_day', 'line_status', 'load_p', 'load_q',\n",
        "    'load_theta', 'load_v', 'max_step', 'minute_of_hour', 'month', 'p_ex', 'p_or',\n",
        "    'prod_p', 'prod_q', 'prod_v', 'q_ex', 'q_or', 'rho', 'target_dispatch',\n",
        "    'thermal_limit', 'theta_ex', 'theta_or', 'time_before_cooldown_line',\n",
        "    'time_before_cooldown_sub', 'time_next_maintenance', 'time_since_last_alarm',\n",
        "     'timestep_overflow', 'topo_vect',\n",
        "     'v_ex', 'v_or','year'\n",
        "]\n",
        "\n",
        "attr_to_keep_no_storage = ['a_ex', 'a_or', 'active_alert', 'actual_dispatch', 'alert_duration',\n",
        "    'attention_budget', 'current_step','curtailment', 'curtailment_limit', 'curtailment_limit_effective', 'curtailment_limit_mw','day',\n",
        "    'day_of_week', 'delta_time', 'duration_next_maintenance', 'gen_margin_down',\n",
        "    'gen_margin_up', 'gen_p', 'gen_p_before_curtail', 'gen_q', 'gen_theta', 'gen_v',\n",
        "    'hour_of_day', 'last_alarm', 'line_status', 'load_p', 'load_q',\n",
        "    'load_theta', 'load_v', 'max_step', 'minute_of_hour', 'month', 'p_ex', 'p_or',\n",
        "    'prod_p', 'prod_q', 'prod_v', 'q_ex', 'q_or', 'rho',\n",
        "     'target_dispatch',\n",
        "    'thermal_limit', 'theta_ex', 'theta_or', 'time_before_cooldown_line',\n",
        "    'time_before_cooldown_sub', 'time_next_maintenance',\n",
        "    'timestep_overflow', 'topo_vect','v_ex', 'v_or',\n",
        "     'year']#so far the best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1z6ahaOZ1wCb",
        "outputId": "e9e13b59-8349-48b8-c0b0-621b194699b7"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.spaces import Discrete, MultiDiscrete, Box\n",
        "\n",
        "import grid2op\n",
        "from grid2op import gym_compat\n",
        "from grid2op.Parameters import Parameters\n",
        "from grid2op.Action import PlayableAction\n",
        "from grid2op.Observation import CompleteObservation\n",
        "from grid2op.Reward import L2RPNReward, N1Reward, CombinedScaledReward\n",
        "from grid2op.gym_compat import DiscreteActSpace, BoxGymObsSpace\n",
        "\n",
        "from lightsim2grid import LightSimBackend\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Gymnasium environment wrapper around Grid2Op environment\n",
        "class Gym2OpEnv(gym.Env):\n",
        "    def __init__(self, attr_to_keep):\n",
        "        super().__init__()\n",
        "\n",
        "        self._backend = LightSimBackend()\n",
        "        self._env_name = \"l2rpn_case14_sandbox\"  # DO NOT CHANGE\n",
        "\n",
        "        action_class = PlayableAction\n",
        "        observation_class = CompleteObservation\n",
        "        reward_class = CombinedScaledReward  # Setup further below\n",
        "\n",
        "        # DO NOT CHANGE Parameters\n",
        "        # See https://grid2op.readthedocs.io/en/latest/parameters.html\n",
        "        p = Parameters()\n",
        "        p.MAX_SUB_CHANGED = 4  # Up to 4 substations can be reconfigured each timestep\n",
        "        p.MAX_LINE_STATUS_CHANGED = 4  # Up to 4 powerline statuses can be changed each timestep\n",
        "\n",
        "        # Make grid2op env\n",
        "        self._g2op_env = grid2op.make(\n",
        "            self._env_name, backend=self._backend, test=False,\n",
        "            action_class=action_class, observation_class=observation_class,\n",
        "            reward_class=reward_class, param=p\n",
        "        )\n",
        "\n",
        "        ##########\n",
        "        # REWARD #\n",
        "        ##########\n",
        "        # NOTE: This reward should not be modified when evaluating RL agent\n",
        "        # See https://grid2op.readthedocs.io/en/latest/reward.html\n",
        "        cr = self._g2op_env.get_reward_instance()\n",
        "        cr.addReward(\"N1\", N1Reward(), 1.0)\n",
        "        cr.addReward(\"L2RPN\", L2RPNReward(), 1.0)\n",
        "        # reward = N1 + L2RPN\n",
        "        cr.initialize(self._g2op_env)\n",
        "        ##########\n",
        "\n",
        "        self._gym_env = gym_compat.GymEnv(self._g2op_env)\n",
        "\n",
        "        self.setup_observations()\n",
        "        self.setup_actions()\n",
        "\n",
        "        self.observation_space = self._gym_env.observation_space\n",
        "        self.action_space = self._gym_env.action_space\n",
        "        self.attr_to_keep = attr_to_keep\n",
        "\n",
        "    # The information i used to get the code for the below 2 functions is fromm the getting started from the Grid2Op github.\n",
        "    # specifcally theis link was useful at helping change the observation and action spaces to use with gymnasium : https://github.com/rte-france/Grid2Op/blob/c71a2dfb824dae7115394266e02cc673c8633a0e/getting_started/11_IntegrationWithExistingRLFrameworks.ipynb\n",
        "    # these links also help explain the observation and action space:\n",
        "        # https://github.com/rte-france/Grid2Op/blob/c71a2dfb824dae7115394266e02cc673c8633a0e/getting_started/02_Observation.ipynb\n",
        "        # https://github.com/rte-france/Grid2Op/blob/c71a2dfb824dae7115394266e02cc673c8633a0e/getting_started/03_Action.ipynb\n",
        "\n",
        "    def setup_observations(self):\n",
        "        \n",
        "        self._gym_env.observation_space.close()\n",
        "        #Observation space = attr_to_keep_no_storage\n",
        "        self._gym_env.observation_space = BoxGymObsSpace(self._g2op_env.observation_space, attr_to_keep = attr_to_keep)\n",
        "       \n",
        "\n",
        "    def setup_actions(self):\n",
        "        \n",
        "        #using the set action space\n",
        "        self._gym_env.action_space = DiscreteActSpace(self._g2op_env.action_space, attr_to_keep=['set_bus', 'set_dispatch'])\n",
        "        \n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        return self._gym_env.reset(seed=seed, options=None)\n",
        "\n",
        "    def step(self, action):\n",
        "        return self._gym_env.step(action)\n",
        "\n",
        "    def render(self):\n",
        "        \n",
        "        return self._gym_env.render()\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Random agent interacting in environment #\n",
        "\n",
        "    max_steps = 100\n",
        "\n",
        "    env = Gym2OpEnv(attr_to_keep_3)\n",
        "\n",
        "    print(\"#####################\")\n",
        "    print(\"# OBSERVATION SPACE #\")\n",
        "    print(\"#####################\")\n",
        "    print(env.observation_space)\n",
        "    print(\"#####################\\n\")\n",
        "\n",
        "    print(\"#####################\")\n",
        "    print(\"#   ACTION SPACE    #\")\n",
        "    print(\"#####################\")\n",
        "    print(env.action_space)\n",
        "    print(\"#####################\\n\\n\")\n",
        "\n",
        "    curr_step = 0\n",
        "    curr_return = 0\n",
        "\n",
        "    is_done = False\n",
        "    obs, info = env.reset()\n",
        "    print(f\"step = {curr_step} (reset):\")\n",
        "    print(f\"\\t obs = {obs}\")\n",
        "    print(f\"\\t info = {info}\\n\\n\")\n",
        "\n",
        "    while not is_done and curr_step < max_steps:\n",
        "        action = env.action_space.sample()\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "        curr_step += 1\n",
        "        curr_return += reward\n",
        "        is_done = terminated or truncated\n",
        "\n",
        "        print(f\"step = {curr_step}: \")\n",
        "        print(f\"\\t obs = {obs}\")\n",
        "        print(f\"\\t reward = {reward}\")\n",
        "        print(f\"\\t terminated = {terminated}\")\n",
        "        print(f\"\\t truncated = {truncated}\")\n",
        "        print(f\"\\t info = {info}\")\n",
        "\n",
        "        # Some actions are invalid (see: https://grid2op.readthedocs.io/en/latest/action.html#illegal-vs-ambiguous)\n",
        "        # Invalid actions are replaced with 'do nothing' action\n",
        "        is_action_valid = not (info[\"is_illegal\"] or info[\"is_ambiguous\"])\n",
        "        print(f\"\\t is action valid = {is_action_valid}\")\n",
        "        if not is_action_valid:\n",
        "            print(f\"\\t\\t reason = {info['exception']}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "    print(\"###########\")\n",
        "    print(\"# SUMMARY #\")\n",
        "    print(\"###########\")\n",
        "    print(f\"return = {curr_return}\")\n",
        "    print(f\"total steps = {curr_step}\")\n",
        "    print(\"###########\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7Cbw1rm1ycW",
        "outputId": "77759606-a261-4311-b9f2-421294258c88"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "# Custom Action Tracker Callback\n",
        "class ActionTrackerCallback(BaseCallback):\n",
        "    def __init__(self, verbose=0):\n",
        "        super(ActionTrackerCallback, self).__init__(verbose)\n",
        "        self.action_counter = defaultdict(int)\n",
        "\n",
        "    def _on_step(self):\n",
        "        # Track the actions taken during training\n",
        "        action = self.locals['actions']\n",
        "        if isinstance(action, np.ndarray):\n",
        "            action_key = tuple(action)\n",
        "        else:\n",
        "            action_key = (action,)\n",
        "        self.action_counter[action_key] += 1\n",
        "        return True\n",
        "\n",
        "    def _on_training_end(self):\n",
        "        # Called at the end of training - print action distribution here\n",
        "        print(\"Training completed! Action distribution:\")\n",
        "        for action, freq in sorted(self.action_counter.items(), key=lambda x: x[1], reverse=True):\n",
        "            print(f\"Action: {action}, Frequency: {freq}\")\n",
        "\n",
        "    def get_action_distribution(self):\n",
        "        return dict(self.action_counter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Nn9KLjhbQwU",
        "outputId": "f98d43be-d0a9-4f2c-bc7b-468c241dfef6"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, env, num_episodes=10):\n",
        "    all_episode_rewards = []\n",
        "    all_episode_lengths = []\n",
        "    all_n1_rewards = []\n",
        "    all_l2rpn_rewards = []\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        obs, _ = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        episode_length = 0\n",
        "        n1_reward = 0\n",
        "        l2rpn_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action, _ = model.predict(obs, deterministic=True)\n",
        "            obs, reward, done, _, info = env.step(action)\n",
        "            episode_reward += reward\n",
        "            episode_length += 1\n",
        "\n",
        "            # Extract individual rewards\n",
        "            n1_reward += info.get('rewards', {}).get('N1', 0)\n",
        "            l2rpn_reward += info.get('rewards', {}).get('L2RPN', 0)\n",
        "\n",
        "        all_episode_rewards.append(episode_reward)\n",
        "        all_episode_lengths.append(episode_length)\n",
        "        all_n1_rewards.append(n1_reward)\n",
        "        all_l2rpn_rewards.append(l2rpn_reward)\n",
        "\n",
        "    mean_reward = np.mean(all_episode_rewards)\n",
        "    std_reward = np.std(all_episode_rewards)\n",
        "    mean_length = np.mean(all_episode_lengths)\n",
        "    std_length = np.std(all_episode_lengths)\n",
        "    mean_n1 = np.mean(all_n1_rewards)\n",
        "    std_n1 = np.std(all_n1_rewards)\n",
        "    mean_l2rpn = np.mean(all_l2rpn_rewards)\n",
        "    std_l2rpn = np.std(all_l2rpn_rewards)\n",
        "\n",
        "    return mean_reward, std_reward, mean_length, std_length, mean_n1, std_n1, mean_l2rpn, std_l2rpn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372,
          "referenced_widgets": [
            "b1b4d2eae54b498d8c64992dfaef0242",
            "fa49f116feb14821b3d9c486ad778500"
          ]
        },
        "id": "L1ubVHzn10lj",
        "outputId": "8d93969d-18f7-4eee-a51e-5873135a975f"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import grid2op\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "import wandb\n",
        "import sys\n",
        "sys.path.insert(0,\"./\") # this makes it possible to import the wrapper since the env.py is in it's own folder\n",
        "#from provided_wrapper.env import Gym2OpEnv\n",
        "from stable_baselines3 import DQN\n",
        "from wandb.integration.sb3 import WandbCallback\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "from sb3_contrib import QRDQN\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Initialize environment and monitor\n",
        "    env = Gym2OpEnv(attr_to_keep_no_storage)\n",
        "    env = Monitor(env)\n",
        "\n",
        "    # Initialize Wandb\n",
        "    run = wandb.init(\n",
        "        project=\"RL_project\",\n",
        "        name=\"DQN Improvemen 1\",\n",
        "        sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
        "        monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
        "        save_code=False,  # optional\n",
        "    )\n",
        "\n",
        "    # Initialize DQN model\n",
        "    model = DQN(\"MlpPolicy\", env, verbose=1, device = device, tensorboard_log=\"./dqn_grid2op_tensorboard/\")\n",
        "\n",
        "    # policy_kwargs = dict(n_quantiles=16)\n",
        "    # model2 = QRDQN(\"MlpPolicy\", env, device=device, policy_kwargs=policy_kwargs,tensorboard_log=\"./dqn_grid2op_tensorboard/\")\n",
        "    # #Initialize custom callback to track actions\n",
        "    action_callback = ActionTrackerCallback()\n",
        "\n",
        "    # Training the model with Wandb and custom action tracking callback\n",
        "    model.learn(\n",
        "        total_timesteps=1000000,\n",
        "        log_interval=10,\n",
        "        callback=[WandbCallback(gradient_save_freq=100, model_save_path=f\"models/{run.id}\", verbose=1), action_callback],\n",
        "        progress_bar=True\n",
        "    )\n",
        "\n",
        "    # After training, print the action distribution\n",
        "    action_distribution = action_callback.get_action_distribution()\n",
        "\n",
        "    print(\"\\nFinal Action Distribution:\")\n",
        "    for action, freq in sorted(action_distribution.items(), key=lambda x: x[1], reverse=True):\n",
        "        print(f\"Action: {action}, Frequency: {freq}\")\n",
        "\n",
        "\n",
        "    #Evaluating the trained model\n",
        "    print(\"\\nEvaluating the trained model\")\n",
        "    mean_reward, std_reward, mean_length, std_length, mean_n1, std_n1, mean_l2rpn, std_l2rpn = evaluate_model(model, env, num_episodes=50)\n",
        "    wandb.log({\n",
        "        \"eval/mean_total_reward\": mean_reward,\n",
        "        \"eval/std_total_reward\": std_reward,\n",
        "        \"eval/mean_episode_length\": mean_length,\n",
        "        \"eval/std_episode_length\": std_length,\n",
        "        \"eval/mean_N1_reward\": mean_n1,\n",
        "        \"eval/std_N1_reward\": std_n1,\n",
        "        \"eval/mean_L2RPN_reward\": mean_l2rpn,\n",
        "        \"eval/std_L2RPN_reward\": std_l2rpn\n",
        "    })\n",
        "\n",
        "    print(f\"\\nEvaluation Results:\")\n",
        "    print(f\"Mean Total Reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "    print(f\"Mean Episode Length: {mean_length:.2f} +/- {std_length:.2f}\")\n",
        "    print(f\"Mean N1 Reward: {mean_n1:.2f} +/- {std_n1:.2f}\")\n",
        "    print(f\"Mean L2RPN Reward: {mean_l2rpn:.2f} +/- {std_l2rpn:.2f}\")\n",
        "    run.finish()\n",
        "\n",
        "    # Test the trained model\n",
        "    obs, _ = env.reset()  # Get initial observation and info\n",
        "    for _ in range(1000):\n",
        "        # Predict action using only the observation part of the returned tuple\n",
        "        action, _states = model.predict(obs, deterministic=True)\n",
        "        obs, reward, done, _, info = env.step(action)\n",
        "        env.render()\n",
        "        if done:\n",
        "            obs, _ = env.reset()  # Reset environment if done, and unpack the tuple again\n",
        "\n",
        "    env.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b1b4d2eae54b498d8c64992dfaef0242": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_fa49f116feb14821b3d9c486ad778500",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">   0%</span> <span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">247/1,000,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:00:14</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">16:09:19</span> , <span style=\"color: #800000; text-decoration-color: #800000\">17 it/s</span> ]\n</pre>\n",
                  "text/plain": "\u001b[35m   0%\u001b[0m \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247/1,000,000 \u001b[0m [ \u001b[33m0:00:14\u001b[0m < \u001b[36m16:09:19\u001b[0m , \u001b[31m17 it/s\u001b[0m ]\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "fa49f116feb14821b3d9c486ad778500": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
