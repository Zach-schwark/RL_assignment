{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QR-DQN - Improvement 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium Grid2Op lightsim2grid wandb stable-baselines3[extra] sb3-contrib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_to_keep_no_storage = ['a_ex', 'a_or', 'active_alert', 'actual_dispatch', 'alert_duration',\n",
    "    'attention_budget', 'current_step','curtailment', 'curtailment_limit', 'curtailment_limit_effective', 'curtailment_limit_mw','day',\n",
    "    'day_of_week', 'delta_time', 'duration_next_maintenance', 'gen_margin_down',\n",
    "    'gen_margin_up', 'gen_p', 'gen_p_before_curtail', 'gen_q', 'gen_theta', 'gen_v',\n",
    "    'hour_of_day', 'last_alarm', 'line_status', 'load_p', 'load_q',\n",
    "    'load_theta', 'load_v', 'max_step', 'minute_of_hour', 'month', 'p_ex', 'p_or',\n",
    "    'prod_p', 'prod_q', 'prod_v', 'q_ex', 'q_or', 'rho',\n",
    "     'target_dispatch',\n",
    "    'thermal_limit', 'theta_ex', 'theta_or', 'time_before_cooldown_line',\n",
    "    'time_before_cooldown_sub', 'time_next_maintenance',\n",
    "    'timestep_overflow', 'topo_vect','v_ex', 'v_or',\n",
    "     'year']#so far the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment - Grid2Op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, MultiDiscrete, Box\n",
    "\n",
    "import grid2op\n",
    "from grid2op import gym_compat\n",
    "from grid2op.Parameters import Parameters\n",
    "from grid2op.Action import PlayableAction\n",
    "from grid2op.Observation import CompleteObservation\n",
    "from grid2op.Reward import L2RPNReward, N1Reward, CombinedScaledReward\n",
    "from grid2op.gym_compat import DiscreteActSpace, BoxGymObsSpace\n",
    "\n",
    "from lightsim2grid import LightSimBackend\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Gymnasium environment wrapper around Grid2Op environment\n",
    "class Gym2OpEnv(gym.Env):\n",
    "    def __init__(self, attr_to_keep):\n",
    "        super().__init__()\n",
    "\n",
    "        self._backend = LightSimBackend()\n",
    "        self._env_name = \"l2rpn_case14_sandbox\"  # DO NOT CHANGE\n",
    "\n",
    "        action_class = PlayableAction\n",
    "        observation_class = CompleteObservation\n",
    "        reward_class = CombinedScaledReward  # Setup further below\n",
    "\n",
    "        # DO NOT CHANGE Parameters\n",
    "        # See https://grid2op.readthedocs.io/en/latest/parameters.html\n",
    "        p = Parameters()\n",
    "        p.MAX_SUB_CHANGED = 4  # Up to 4 substations can be reconfigured each timestep\n",
    "        p.MAX_LINE_STATUS_CHANGED = 4  # Up to 4 powerline statuses can be changed each timestep\n",
    "\n",
    "        # Make grid2op env\n",
    "        self._g2op_env = grid2op.make(\n",
    "            self._env_name, backend=self._backend, test=False,\n",
    "            action_class=action_class, observation_class=observation_class,\n",
    "            reward_class=reward_class, param=p\n",
    "        )\n",
    "\n",
    "        ##########\n",
    "        # REWARD #\n",
    "        ##########\n",
    "        # NOTE: This reward should not be modified when evaluating RL agent\n",
    "        # See https://grid2op.readthedocs.io/en/latest/reward.html\n",
    "        cr = self._g2op_env.get_reward_instance()\n",
    "        cr.addReward(\"N1\", N1Reward(), 1.0)\n",
    "        cr.addReward(\"L2RPN\", L2RPNReward(), 1.0)\n",
    "        # reward = N1 + L2RPN\n",
    "        cr.initialize(self._g2op_env)\n",
    "        ##########\n",
    "\n",
    "        self._gym_env = gym_compat.GymEnv(self._g2op_env)\n",
    "\n",
    "        self.setup_observations()\n",
    "        self.setup_actions()\n",
    "\n",
    "        self.observation_space = self._gym_env.observation_space\n",
    "        self.action_space = self._gym_env.action_space\n",
    "        self.attr_to_keep = attr_to_keep\n",
    "\n",
    "    # The information i used to get the code for the below 2 functions is fromm the getting started from the Grid2Op github.\n",
    "    # specifcally theis link was useful at helping change the observation and action spaces to use with gymnasium : https://github.com/rte-france/Grid2Op/blob/c71a2dfb824dae7115394266e02cc673c8633a0e/getting_started/11_IntegrationWithExistingRLFrameworks.ipynb\n",
    "    # these links also help explain the observation and action space:\n",
    "        # https://github.com/rte-france/Grid2Op/blob/c71a2dfb824dae7115394266e02cc673c8633a0e/getting_started/02_Observation.ipynb\n",
    "        # https://github.com/rte-france/Grid2Op/blob/c71a2dfb824dae7115394266e02cc673c8633a0e/getting_started/03_Action.ipynb\n",
    "\n",
    "    def setup_observations(self):\n",
    "        \n",
    "        self._gym_env.observation_space.close()\n",
    "        #Observation space = attr_to_keep_no_storage\n",
    "        self._gym_env.observation_space = BoxGymObsSpace(self._g2op_env.observation_space, attr_to_keep = attr_to_keep)\n",
    "       \n",
    "\n",
    "    def setup_actions(self):\n",
    "        \n",
    "        #using the full action space for the QR-DQN distribution\n",
    "        self._gym_env.action_space = DiscreteActSpace(self._g2op_env.action_space)\n",
    "        \n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        return self._gym_env.reset(seed=seed, options=None)\n",
    "\n",
    "    def step(self, action):\n",
    "        return self._gym_env.step(action)\n",
    "\n",
    "    def render(self):\n",
    "        \n",
    "        return self._gym_env.render()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Random agent interacting in environment #\n",
    "\n",
    "    max_steps = 100\n",
    "\n",
    "    env = Gym2OpEnv(attr_to_keep_3)\n",
    "\n",
    "    print(\"#####################\")\n",
    "    print(\"# OBSERVATION SPACE #\")\n",
    "    print(\"#####################\")\n",
    "    print(env.observation_space)\n",
    "    print(\"#####################\\n\")\n",
    "\n",
    "    print(\"#####################\")\n",
    "    print(\"#   ACTION SPACE    #\")\n",
    "    print(\"#####################\")\n",
    "    print(env.action_space)\n",
    "    print(\"#####################\\n\\n\")\n",
    "\n",
    "    curr_step = 0\n",
    "    curr_return = 0\n",
    "\n",
    "    is_done = False\n",
    "    obs, info = env.reset()\n",
    "    print(f\"step = {curr_step} (reset):\")\n",
    "    print(f\"\\t obs = {obs}\")\n",
    "    print(f\"\\t info = {info}\\n\\n\")\n",
    "\n",
    "    while not is_done and curr_step < max_steps:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        curr_step += 1\n",
    "        curr_return += reward\n",
    "        is_done = terminated or truncated\n",
    "\n",
    "        print(f\"step = {curr_step}: \")\n",
    "        print(f\"\\t obs = {obs}\")\n",
    "        print(f\"\\t reward = {reward}\")\n",
    "        print(f\"\\t terminated = {terminated}\")\n",
    "        print(f\"\\t truncated = {truncated}\")\n",
    "        print(f\"\\t info = {info}\")\n",
    "\n",
    "        # Some actions are invalid (see: https://grid2op.readthedocs.io/en/latest/action.html#illegal-vs-ambiguous)\n",
    "        # Invalid actions are replaced with 'do nothing' action\n",
    "        is_action_valid = not (info[\"is_illegal\"] or info[\"is_ambiguous\"])\n",
    "        print(f\"\\t is action valid = {is_action_valid}\")\n",
    "        if not is_action_valid:\n",
    "            print(f\"\\t\\t reason = {info['exception']}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    print(\"###########\")\n",
    "    print(\"# SUMMARY #\")\n",
    "    print(\"###########\")\n",
    "    print(f\"return = {curr_return}\")\n",
    "    print(f\"total steps = {curr_step}\")\n",
    "    print(\"###########\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "# Custom Action Tracker Callback\n",
    "class ActionTrackerCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(ActionTrackerCallback, self).__init__(verbose)\n",
    "        self.action_counter = defaultdict(int)\n",
    "\n",
    "    def _on_step(self):\n",
    "        # Track the actions taken during training\n",
    "        action = self.locals['actions']\n",
    "        if isinstance(action, np.ndarray):\n",
    "            action_key = tuple(action)\n",
    "        else:\n",
    "            action_key = (action,)\n",
    "        self.action_counter[action_key] += 1\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        # Called at the end of training - print action distribution here\n",
    "        print(\"Training completed! Action distribution:\")\n",
    "        for action, freq in sorted(self.action_counter.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"Action: {action}, Frequency: {freq}\")\n",
    "\n",
    "    def get_action_distribution(self):\n",
    "        return dict(self.action_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, env, num_episodes=10):\n",
    "    all_episode_rewards = []\n",
    "    all_episode_lengths = []\n",
    "    all_n1_rewards = []\n",
    "    all_l2rpn_rewards = []\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        n1_reward = 0\n",
    "        l2rpn_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "\n",
    "            # Extract individual rewards\n",
    "            n1_reward += info.get('rewards', {}).get('N1', 0)\n",
    "            l2rpn_reward += info.get('rewards', {}).get('L2RPN', 0)\n",
    "\n",
    "        all_episode_rewards.append(episode_reward)\n",
    "        all_episode_lengths.append(episode_length)\n",
    "        all_n1_rewards.append(n1_reward)\n",
    "        all_l2rpn_rewards.append(l2rpn_reward)\n",
    "\n",
    "    mean_reward = np.mean(all_episode_rewards)\n",
    "    std_reward = np.std(all_episode_rewards)\n",
    "    mean_length = np.mean(all_episode_lengths)\n",
    "    std_length = np.std(all_episode_lengths)\n",
    "    mean_n1 = np.mean(all_n1_rewards)\n",
    "    std_n1 = np.std(all_n1_rewards)\n",
    "    mean_l2rpn = np.mean(all_l2rpn_rewards)\n",
    "    std_l2rpn = np.std(all_l2rpn_rewards)\n",
    "\n",
    "    return mean_reward, std_reward, mean_length, std_length, mean_n1, std_n1, mean_l2rpn, std_l2rpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import grid2op\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import wandb\n",
    "import sys\n",
    "sys.path.insert(0,\"./\") # this makes it possible to import the wrapper since the env.py is in it's own folder\n",
    "#from provided_wrapper.env import Gym2OpEnv\n",
    "from stable_baselines3 import DQN\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sb3_contrib import QRDQN\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Initialize environment and monitor\n",
    "    env = Gym2OpEnv(attr_to_keep_no_storage)\n",
    "    env = Monitor(env)\n",
    "\n",
    "    # Initialize Wandb\n",
    "    run = wandb.init(\n",
    "        project=\"RL_project\",\n",
    "        name=\"DQN Improvemen 1\",\n",
    "        sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "        monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
    "        save_code=False,  # optional\n",
    "    )\n",
    "\n",
    "    # Initialize QR-DQN model\n",
    "    \n",
    "\n",
    "    policy_kwargs = dict(n_quantiles=8)\n",
    "    model2 = QRDQN(\"MlpPolicy\", env, device=device, policy_kwargs=policy_kwargs,tensorboard_log=\"./dqn_grid2op_tensorboard/\")\n",
    "    #Initialize custom callback to track actions\n",
    "    action_callback = ActionTrackerCallback()\n",
    "\n",
    "    # Training the model with Wandb and custom action tracking callback\n",
    "    model2.learn(\n",
    "        total_timesteps=1000000,\n",
    "        log_interval=10,\n",
    "        callback=[WandbCallback(gradient_save_freq=100, model_save_path=f\"models/{run.id}\", verbose=1), action_callback],\n",
    "        progress_bar=True\n",
    "    )\n",
    "\n",
    "    # After training, print the action distribution\n",
    "    action_distribution = action_callback.get_action_distribution()\n",
    "\n",
    "    print(\"\\nFinal Action Distribution:\")\n",
    "    for action, freq in sorted(action_distribution.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"Action: {action}, Frequency: {freq}\")\n",
    "\n",
    "\n",
    "    #Evaluating the trained model\n",
    "    print(\"\\nEvaluating the trained model\")\n",
    "    mean_reward, std_reward, mean_length, std_length, mean_n1, std_n1, mean_l2rpn, std_l2rpn = evaluate_model(model2, env, num_episodes=50)\n",
    "    wandb.log({\n",
    "        \"eval/mean_total_reward\": mean_reward,\n",
    "        \"eval/std_total_reward\": std_reward,\n",
    "        \"eval/mean_episode_length\": mean_length,\n",
    "        \"eval/std_episode_length\": std_length,\n",
    "        \"eval/mean_N1_reward\": mean_n1,\n",
    "        \"eval/std_N1_reward\": std_n1,\n",
    "        \"eval/mean_L2RPN_reward\": mean_l2rpn,\n",
    "        \"eval/std_L2RPN_reward\": std_l2rpn\n",
    "    })\n",
    "\n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\"Mean Total Reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    print(f\"Mean Episode Length: {mean_length:.2f} +/- {std_length:.2f}\")\n",
    "    print(f\"Mean N1 Reward: {mean_n1:.2f} +/- {std_n1:.2f}\")\n",
    "    print(f\"Mean L2RPN Reward: {mean_l2rpn:.2f} +/- {std_l2rpn:.2f}\")\n",
    "    run.finish()\n",
    "\n",
    "    # Test the trained model\n",
    "    obs, _ = env.reset()  # Get initial observation and info\n",
    "    for _ in range(1000):\n",
    "        # Predict action using only the observation part of the returned tuple\n",
    "        action, _states = model2.predict(obs, deterministic=True)\n",
    "        obs, reward, done, _, info = env.step(action)\n",
    "        env.render()\n",
    "        if done:\n",
    "            obs, _ = env.reset()  # Reset environment if done, and unpack the tuple again\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
